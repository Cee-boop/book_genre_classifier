{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fb547c8-759b-4697-be23-b043422e2974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "_\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from unicodedata import normalize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "_\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score, log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea1f482-4d45-4a9a-b60b-80abb528646e",
   "metadata": {},
   "source": [
    "A Bunch of help functions I'll use throughout the project. I thought it would be easier to keep them all in one notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0725299-23b2-4fce-ab81-69f8da94eee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_words() -> set:\n",
    "    global list_of_authors\n",
    "    \"\"\"\n",
    "    Compile stop words to remove from text.\n",
    "    \"\"\"\n",
    "    custom_words = [\n",
    "        'edition', 'isbn', 'alternate', 'cover', 'one', 'story', 'book', 'novel', 'page', 'pages', 'new', 'latest', 'written', 'hyped', 'tiktok',\n",
    "        'cover', 'note', 'bestseller', 'newest', 'list', 'print', 'ever', 'tale', 'author', 'reader', 'work', 'literary', 'epic', 'fantasy', \n",
    "        'scifi', 'romance', 'fiction', 'nonfiction', 'thriller', 'paperback', 'hardback', 'contemporary', 'science', 'character', 'epic', \n",
    "        'series', 'turner', 'classic', 'plot', 'synopsis', 'blurb', 'collection', 'short', 'writer', 'writing', 'reader', 'reprint', 'librarian'\n",
    "        'print', 'chapter', 'published', 'paper', 'anticipated', 'science fiction', 'beautiful' 'type', 'stories', 'genre', 'write', 'award',\n",
    "        'winning', 'debut', 'masterpiece', 'youtube', 'goodreads', 'prize', 'booker', 'pulitzer', 'hugo', 'longlist', 'shortlist', 'acclaim',\n",
    "        'critcally', 'adapation', 'tv', 'show', 'motion', 'picture', 'netflix', 'streaming', 'stream', 'best', 'woman', 'man', 'family', 'know',\n",
    "        'time', 'winner', 'thats', 'youve', 'youre'\n",
    "    ] \n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    # Appened custom_words to stop_words\n",
    "    for word in custom_words:\n",
    "        stop_words.append(word)\n",
    "\n",
    "    return set(stop_words)\n",
    "\n",
    "\n",
    "def clean_text(text: str, get_word_count=False) -> str | int:\n",
    "    \"\"\"\n",
    "    Remove special symbols, punctuation, and specific patterns in text.\n",
    "    \"\"\"\n",
    "    if get_word_count:\n",
    "        # Returns word count for text\n",
    "        text = re.sub(r'[^\\w ]+', ' ', text)  # Remove special symbols\n",
    "        text = re.sub(\"\\s+\", ' ', text)  # Remove additional whitespaces\n",
    "        return len(text.split(' '))\n",
    "    \n",
    "    stop_words = get_stop_words()\n",
    "    text = text.lower()\n",
    "    text = text.strip()    \n",
    "    text = \" \". join([word for word in text.split(' ') if word not in stop_words])  # Remove stop words before processing individual words\n",
    "    text = re.sub('<.*?>', '', text)  # Remove HTML tags\n",
    "    text = normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')  # Remove accented characters\n",
    "    text = re.sub(r\"[0-9]\", '', text)  # Removes numbers\n",
    "    text = re.sub(r'[^\\w ]+', ' ', text)  # Remove special symbols\n",
    "    text = re.sub(\"\\s+\", ' ', text)  # Remove additional whitespaces\n",
    "    text = re.sub(r'[_]', '', text)  # Remove underscores\n",
    "    return text\n",
    "    \n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Tokenize, lemmatize, and remove stop words from text.\n",
    "    \"\"\"\n",
    "    stop_words = get_stop_words()\n",
    "    lem = WordNetLemmatizer()\n",
    "    \n",
    "    for _ in range(2):\n",
    "        text = word_tokenize(text)  # Tokenize text\n",
    "        text = [lem.lemmatize(word) for word in text]  # Lemmatize text\n",
    "        text = [word for word in text if len(word) > 2]  # Remove words with a length of 2 or less\n",
    "        # REMOVE STOP WORDS ------------------------------------------------------\n",
    "        text = [word for word in text if word not in stop_words]\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def tdm_converter(vect: object, docs: list) -> np.array:\n",
    "    \"\"\"\n",
    "    Convert vectorized documents into a text document matrix.\n",
    "    \"\"\"\n",
    "    vect_transformer = vect.transform(docs)\n",
    "    return vect_transformer.toarray()\n",
    "\n",
    "\n",
    "def display_metrics(model_name: str, y_true: list, y_hat: list, y_hat_probas: list):\n",
    "    \"\"\"\n",
    "    Displays numerous metrics about current model predictions.\n",
    "    \"\"\"\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy score: {accuracy_score(y_true, y_hat)}\")\n",
    "    print(f\"Recall score: {recall_score(y_true, y_hat, average='macro')}\")\n",
    "    print(f\"Precision score: {precision_score(y_true, y_hat, average='macro')}\")\n",
    "    print(f\"f1 score: {f1_score(y_true, y_hat, average='macro')}\")\n",
    "    try:\n",
    "        print(f\"Log loss {log_loss(y_true, y_hat_probas, labels=y_true)}\")\n",
    "        print(f\"ROC AUC: {roc_auc_score(y_true, y_hat_probas, multi_class='ovr')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fullstack",
   "language": "python",
   "name": "fullstack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
